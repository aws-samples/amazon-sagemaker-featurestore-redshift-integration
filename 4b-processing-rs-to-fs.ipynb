{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ba289e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ingest Data with SageMaker Processing Job\n",
    "\n",
    "This notebook demonstrates how to set up a SageMaker Processing Job with AWS pre-built image to leverage Feature Store Spark Connector. This solution is for any practitioner who wants to utilize SageMaker as one-stop platform, to ingest and process features from Redshift to SageMaker Feature Store. \n",
    "\n",
    "Here we use USER DATA as an example, for your reference. You can try PLACE and RATING DATA according to following implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177eb385",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "Your can use SageMaker Stuio, Notebook Instance, AWS Cloud9 or your own local environment to run following codes. Please make sure related AWS credentials are configured well with enough IAM policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3b250",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Spark script in SageMaker Processing Job\n",
    "\n",
    "Prepare a Spark script to retrieve dataset from Redshift, do feature engineering and ingest features into Feature Store. Here we use Redshift Dataset Definition in SageMaker Processing API as Processing Input and use Spark to read dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02794498-ae83-46c6-8504-dd0036596ac1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ETL Script for User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb204b4-3547-42bd-8c52-7a2438fe2646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b091765",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./code/processing_etl_user.py\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    " \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import base64\n",
    "import collections\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "# from contextlib import redirect_stdout\n",
    "from datetime import date\n",
    "from enum import Enum\n",
    "from io import BytesIO\n",
    "from pyspark.sql import functions as sf, types, Column\n",
    "from time import gmtime, strftime, sleep\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import udf, pandas_udf, to_timestamp\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    FractionalType,\n",
    "    IntegralType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    NumericType\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as fn\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import rand,when\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import subprocess\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Let's first install Feature Store Spark Connector library.\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',  'sagemaker-feature-store-pyspark-3.1', '--no-binary', ':all:', '--verbose'])\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--s3-input-path\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--job-name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--feature-group-arn\", type=str, default=\"\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    # Combine S3 Path for Redshift Dataset\n",
    "    _s3_input_path = \"{}/{}/data/\".format(args.s3_input_path, args.job_name)\n",
    "    \n",
    "    print(\"_s3_input_path {}\".format(_s3_input_path))\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # S3 directory for storing Redshift Dataset\n",
    "    _df = spark.read.parquet(_s3_input_path)\n",
    "    \n",
    "    print(_df.dtypes)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    users_data = _df.withColumn(\"timestamp\", date_format(col(\"timestamp\"),\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")) \n",
    "    # Drop Columns\n",
    "    drop_cols = ['user_activity','user_ambience','user_birth_year','user_cuisine','user_color','user_dress_preference','user_hijos','user_dress_preference','user_marital_status','user_payment','user_weight','user_religion','user_height']\n",
    "    users_data = users_data.drop(*drop_cols)\n",
    "    # Cast boolean to string\n",
    "    users_data = users_data.withColumn(\"user_smoker\", users_data[\"user_smoker\"].cast(\"string\"))\n",
    "    print(_df.dtypes)\n",
    "    \n",
    "    # Fill missing values\n",
    "    # For User data\n",
    "    users_data=users_data.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in users_data.columns])\n",
    "    # get string&boolean col name list\n",
    "    string_col = [item[0] for item in users_data.dtypes if item[1].startswith('string')]\n",
    "    string_col.remove('timestamp')\n",
    "    # replace all empty values with null\n",
    "    users_data=users_data.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in users_data.columns])\n",
    "    # calculate means for non-string column\n",
    "    for_missing_value = users_data.agg(*[fn.mean(c).alias(c) for c in users_data.columns if c not in string_col]).toPandas().to_dict('records')[0]\n",
    "    # get mode value for string volumn\n",
    "    for_missing_value.update(dict(zip(string_col, [users_data.groupby(i).count().orderBy(\"count\", ascending=False).first()[0] if users_data.groupby(i).count().orderBy(\"count\", ascending=False).first()[0] is not None else users_data.groupby(i).count().orderBy(\"count\", ascending=False).first()[1] for i in string_col])))\n",
    "    # fill with missing values\n",
    "    users_data = users_data.fillna(for_missing_value)\n",
    "    users_data.show(5)\n",
    "    \n",
    "    # Ordinal Encoding\n",
    "    user_drink_level_dict = {'ABSTEMIOUS': 0, 'CASUAL DRINKER': 1, 'SOCIAL DRINKER': 2}\n",
    "    user_budget_dict = {'LOW': 0, 'MEDIUM': 1, 'HIGH': 2}\n",
    "    user_transport_dict = {'ON FOOT': 0, 'PUBLIC': 1, 'CAR OWNER': 2}\n",
    "    # user drink\n",
    "    keys = array(list(map(lit, user_drink_level_dict.keys())))\n",
    "    values = array(list(map(lit, user_drink_level_dict.values())))\n",
    "    _map = map_from_arrays(keys, values)\n",
    "    users_data = users_data.withColumn(\"user_drink_level2\", _map.getItem(col(\"user_drink_level\"))).drop(\"user_drink_level\").withColumnRenamed(\"user_drink_level2\", \"user_drink_level\")\n",
    "    # user budget\n",
    "    keys = array(list(map(lit, user_budget_dict.keys())))\n",
    "    values = array(list(map(lit, user_budget_dict.values())))\n",
    "    _map = map_from_arrays(keys, values)\n",
    "    users_data = users_data.withColumn(\"user_budget2\", _map.getItem(col(\"user_budget\"))).drop(\"user_budget\").withColumnRenamed(\"user_budget2\", \"user_budget\")\n",
    "    # user transport\n",
    "    keys = array(list(map(lit, user_transport_dict.keys())))\n",
    "    values = array(list(map(lit, user_transport_dict.values())))\n",
    "    _map = map_from_arrays(keys, values)\n",
    "    users_data = users_data.withColumn(\"user_transport2\", _map.getItem(col(\"user_transport\"))).drop(\"user_transport\").withColumnRenamed(\"user_transport2\", \"user_transport\")\n",
    "    # One-hot Encoding\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    indexer = StringIndexer(inputCol='user_interest', outputCol='user_interest_numeric')\n",
    "    indexer_fitted = indexer.fit(users_data)\n",
    "    users_data = indexer_fitted.transform(users_data)\n",
    "\n",
    "    indexer2 = StringIndexer(inputCol='user_personality', outputCol='user_personality_numeric')\n",
    "    indexer_fitted2 = indexer2.fit(users_data)\n",
    "    users_data = indexer_fitted2.transform(users_data)\n",
    "\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "    encoder = OneHotEncoder(inputCols=['user_interest_numeric','user_personality_numeric'], outputCols=['user_interest_onehot','user_personality_onehot'], dropLast=False)\n",
    "    df_onehot = encoder.fit(users_data).transform(users_data)\n",
    "    # For user interest feature\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    df_user_interest_onehot = df_onehot.select('*', vector_to_array('user_interest_onehot').alias('_interest_onehot'))\n",
    "    num_categories = len(df_user_interest_onehot.first()['_interest_onehot'])\n",
    "    cols_expanded = [(F.col('_interest_onehot')[i].alias(f'{indexer_fitted.labels[i]}')) for i in range(num_categories)]\n",
    "    df_user_interest_onehot = df_user_interest_onehot.select('userid',*cols_expanded)\n",
    "    df_user_interest_onehot = df_user_interest_onehot.select('userid',col(\"TECHNOLOGY\").alias(\"user_interest_\" + \"TECHNOLOGY\"), col(\"ECO-FRIENDLY\").alias(\"user_interest_\" + \"ECO-FRIENDLY\"), col(\"NONE\").alias(\"user_interest_\" + \"NONE\"), col(\"VARIETY\").alias(\"user_interest_\" + \"VARIETY\"), col(\"RETRO\").alias(\"user_interest_\" + \"RETRO\"))\n",
    "    # For user personality feature\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    df_user_personality_onehot = df_onehot.select('*', vector_to_array('user_personality_onehot').alias('_personality_onehot'))\n",
    "    num_categories = len(df_user_personality_onehot.first()['_personality_onehot'])\n",
    "    cols_expanded = [(F.col('_personality_onehot')[i].alias(f'{indexer_fitted2.labels[i]}')) for i in range(num_categories)]\n",
    "    df_user_personality_onehot = df_user_personality_onehot.select('userid',*cols_expanded)\n",
    "    df_user_personality_onehot = df_user_personality_onehot.select('userid',col(\"CONFORMIST\").alias(\"user_personality_\" + \"CONFORMIST\"), col(\"THRIFTY-PROTECTOR\").alias(\"user_personality_\" + \"THRIFTY-PROTECTOR\"), col(\"HUNTER-OSTENTATIOUS\").alias(\"user_personality_\" + \"HUNTER-OSTENTATIOUS\"), col(\"HARD-WORKER\").alias(\"user_personality_\" + \"HARD-WORKER\"))\n",
    "    # Join together and drop original features\n",
    "    _col = [\"user_interest\" , \"user_personality\" , \"user_interest_numeric\" ,\"user_personality_numeric\"]\n",
    "    users_data = users_data.drop(*_col).join(df_user_interest_onehot,['userid'],how='inner').join(df_user_personality_onehot,['userid'],how='inner')  \n",
    "    # Reorder columns\n",
    "    _column_names = ['userid', 'user_drink_level', 'user_smoker', 'user_budget', 'user_latitude', 'user_longitude','user_transport','user_interest_VARIETY','user_interest_ECO-FRIENDLY',  'user_interest_RETRO', 'user_interest_TECHNOLOGY', 'user_interest_NONE', 'user_personality_CONFORMIST', 'user_personality_THRIFTY-PROTECTOR', 'user_personality_HUNTER-OSTENTATIOUS', 'user_personality_HARD-WORKER', 'timestamp']\n",
    "    users_data = users_data.select(_column_names)\n",
    "     \n",
    "    users_data.show(5)\n",
    "    \n",
    "    \n",
    "    # Use Feature Store PySpark Library\n",
    "    from feature_store_pyspark.FeatureStoreManager import FeatureStoreManager\n",
    "    import feature_store_pyspark\n",
    "    \n",
    "    feature_store_manager= FeatureStoreManager()\n",
    "    feature_group_arn = args.feature_group_arn\n",
    "    feature_store_manager.ingest_data(input_data_frame=users_data, feature_group_arn=feature_group_arn, target_stores=[\"OfflineStore\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be8cd0-72ec-49fd-a340-64a021b58162",
   "metadata": {},
   "source": [
    "### ETL Script for Place Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b9771-5f9f-4d9a-81d9-c29f7f545f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./code/processing_etl_place.py\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    " \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import base64\n",
    "import collections\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "# from contextlib import redirect_stdout\n",
    "from datetime import date\n",
    "from enum import Enum\n",
    "from io import BytesIO\n",
    "from pyspark.sql import functions as sf, types, Column\n",
    "from time import gmtime, strftime, sleep\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import udf, pandas_udf, to_timestamp\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    FractionalType,\n",
    "    IntegralType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    NumericType\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as fn\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import rand,when\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import subprocess\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Let's first install Feature Store Spark Connector library.\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',  'sagemaker-feature-store-pyspark-3.1', '--no-binary', ':all:', '--verbose'])\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--s3-input-path\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--job-name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--feature-group-arn\", type=str, default=\"\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    # Combine S3 Path for Redshift Dataset\n",
    "    _s3_input_path = \"{}/{}/data/\".format(args.s3_input_path, args.job_name)\n",
    "    \n",
    "    print(\"_s3_input_path {}\".format(_s3_input_path))\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # S3 directory for storing Redshift Dataset\n",
    "    _df = spark.read.parquet(_s3_input_path)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    places_data = _df.withColumn(\"timestamp\", date_format(col(\"timestamp\"),\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")) \n",
    "    # Drop Columns\n",
    "    drop_cols = ['place_address','place_accessibility','place_ambience','place_area','place_city','place_dress_code','place_cuisine','place_franchise','place_name','place_other_services','place_payment','place_state','place_zip','place_country']\n",
    "    places_data = places_data.drop(*drop_cols)\n",
    "    # Ordinal Encoding\n",
    "    place_alcohol_dict = {'NO_ALCOHOL_SERVED': 0, 'WINE-BEER': 1, 'FULL_BAR': 2}\n",
    "    place_smoking_area_dict = {'NONE': 0, 'NOT PERMITTED': 0, 'ONLY AT BAR': 1, 'SECTION': 2, 'PERMITTED': 3}\n",
    "    place_price_dict = {'LOW': 0, 'MEDIUM': 1, 'HIGH': 2}\n",
    "    place_parking_lot_dict = {'[NONE]': 0, '[PUBLIC]': 1, '[VALET PARKING]': 2, '[YES]': 3}\n",
    "\n",
    "    # place_alcohol\n",
    "    keys = array(list(map(lit, place_alcohol_dict.keys())))\n",
    "    values = array(list(map(lit, place_alcohol_dict.values())))\n",
    "    _map = map_from_arrays(keys, values)\n",
    "    places_data = places_data.withColumn(\"place_alcohol2\", _map.getItem(col(\"place_alcohol\"))).drop(\"place_alcohol\").withColumnRenamed(\"place_alcohol2\", \"place_alcohol\")\n",
    "    # place_smoking_area\n",
    "    keys = array(list(map(lit, place_smoking_area_dict.keys())))\n",
    "    values = array(list(map(lit, place_smoking_area_dict.values())))\n",
    "    _map = map_from_arrays(keys, values)\n",
    "    places_data = places_data.withColumn(\"place_smoking_area2\", _map.getItem(col(\"place_smoking_area\"))).drop(\"place_smoking_area\").withColumnRenamed(\"place_smoking_area2\", \"place_smoking_area\")\n",
    "    # place_price\n",
    "    keys = array(list(map(lit, place_price_dict.keys())))\n",
    "    values = array(list(map(lit, place_price_dict.values())))\n",
    "    _map = map_from_arrays(keys, values)\n",
    "    places_data = places_data.withColumn(\"place_price2\", _map.getItem(col(\"place_price\"))).drop(\"place_price\").withColumnRenamed(\"place_price2\", \"place_price\")\n",
    "    # place_parking_lot\n",
    "    keys = array(list(map(lit, place_parking_lot_dict.keys())))\n",
    "    values = array(list(map(lit, place_parking_lot_dict.values())))\n",
    "    _map = map_from_arrays(keys, values)\n",
    "    places_data = places_data.withColumn(\"place_parking_lot2\", _map.getItem(col(\"place_parking_lot\"))).drop(\"place_parking_lot\").withColumnRenamed(\"place_parking_lot2\", \"place_parking_lot\")\n",
    "    # Reorder columns\n",
    "    _column_names = ['placeid', 'place_latitude', 'place_longitude', 'place_smoking_area', 'place_alcohol', 'place_price','place_parking_lot','timestamp']\n",
    "    places_data = places_data.select(_column_names)\n",
    "    \n",
    "    # Use Feature Store PySpark Library\n",
    "    from feature_store_pyspark.FeatureStoreManager import FeatureStoreManager\n",
    "    import feature_store_pyspark\n",
    "    \n",
    "    feature_store_manager= FeatureStoreManager()\n",
    "    feature_group_arn = args.feature_group_arn\n",
    "    feature_store_manager.ingest_data(input_data_frame=places_data, feature_group_arn=feature_group_arn, target_stores=[\"OfflineStore\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fafc13-b776-4638-becd-f1ebc5f8f5e1",
   "metadata": {},
   "source": [
    "### ETL Script for Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d09b7d-79e1-482e-bf91-ddffd9187a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./code/processing_etl_rating.py\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    " \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import base64\n",
    "import collections\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "# from contextlib import redirect_stdout\n",
    "from datetime import date\n",
    "from enum import Enum\n",
    "from io import BytesIO\n",
    "from pyspark.sql import functions as sf, types, Column\n",
    "from time import gmtime, strftime, sleep\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import udf, pandas_udf, to_timestamp\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    FractionalType,\n",
    "    IntegralType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    NumericType\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as fn\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import rand,when\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import subprocess\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Let's first install Feature Store Spark Connector library.\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',  'sagemaker-feature-store-pyspark-3.1', '--no-binary', ':all:', '--verbose'])\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--s3-input-path\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--job-name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--feature-group-arn\", type=str, default=\"\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    # Combine S3 Path for Redshift Dataset\n",
    "    _s3_input_path = \"{}/{}/data/\".format(args.s3_input_path, args.job_name)\n",
    "    \n",
    "    print(\"_s3_input_path {}\".format(_s3_input_path))\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # S3 directory for storing Redshift Dataset\n",
    "    _df = spark.read.parquet(_s3_input_path)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    ratings_data = _df.withColumn(\"timestamp\", date_format(col(\"timestamp\"),\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")) \n",
    "    # Drop Columns\n",
    "    drop_cols = ['rating_food','rating_service']\n",
    "    ratings_data = ratings_data.drop(*drop_cols)\n",
    "    # Reorder columns\n",
    "    _column_names = ['userid', 'ratingid', 'placeid', 'rating_overall','timestamp']\n",
    "    ratings_data = ratings_data.select(_column_names)\n",
    "    \n",
    "    # Use Feature Store PySpark Library\n",
    "    from feature_store_pyspark.FeatureStoreManager import FeatureStoreManager\n",
    "    import feature_store_pyspark\n",
    "    \n",
    "    feature_store_manager= FeatureStoreManager()\n",
    "    feature_group_arn = args.feature_group_arn\n",
    "    feature_store_manager.ingest_data(input_data_frame=ratings_data, feature_group_arn=feature_group_arn, target_stores=[\"OfflineStore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7157294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update sagemaker SDK library\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83938d7",
   "metadata": {},
   "source": [
    "## Prepare jar file dependency of SageMaker Feature Store Spark Connector\n",
    "\n",
    "Here we install SageMaker FeatureStore Spark Connector library in our local environment and retrieve the path of jar file dependency of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfcedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip install sagemaker-feature-store-pyspark-3.1\n",
    "\n",
    "# Get the local path\n",
    "_jar_location = !feature-store-pyspark-dependency-jars\n",
    "_jar_location = _jar_location[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a71e4",
   "metadata": {},
   "source": [
    "## Invoke SageMaker Processing Job\n",
    "\n",
    "Here we provide a Redshift Dataset Definition of designated cluster, database, db_user, query string etc. to filter dataset in specific database and table to processing containers, for further feature engineering and ingestion.\n",
    "\n",
    "First let's retrieve the parameters defined before in Secret Manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448daeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_sagemaker_processing_job(_cluster_id,_dbname,_username,_query_string,_redshift_role_arn,_s3_rdd_output,_processing_job_role_arn,_submit_app,_jar_location,_job_name,_feature_group_arn):\n",
    "\n",
    "    from sagemaker.spark.processing import PySparkProcessor\n",
    "    from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "    from sagemaker.dataset_definition.inputs import (\n",
    "        DatasetDefinition,\n",
    "        RedshiftDatasetDefinition,\n",
    "        AthenaDatasetDefinition,\n",
    "        S3Input,\n",
    "    )\n",
    "    from sagemaker.processing import (\n",
    "        ProcessingInput,\n",
    "        ProcessingOutput,\n",
    "        ScriptProcessor,\n",
    "        Processor,\n",
    "        ProcessingJob,\n",
    "        FeatureStoreOutput,\n",
    "    )\n",
    "\n",
    "    rdd_input = ProcessingInput(\n",
    "                input_name=\"redshift_dataset_definition\",\n",
    "                app_managed=True,\n",
    "                dataset_definition=DatasetDefinition(\n",
    "                    local_path=\"/opt/ml/processing/input/rdd\",\n",
    "                    data_distribution_type=\"FullyReplicated\",\n",
    "                    input_mode=\"File\",\n",
    "                    redshift_dataset_definition=RedshiftDatasetDefinition(\n",
    "                        cluster_id=_cluster_id,\n",
    "                        database=_dbname,\n",
    "                        db_user=_username,\n",
    "                        query_string=_query_string,\n",
    "                        cluster_role_arn=_redshift_role_arn,\n",
    "                        output_s3_uri=_s3_rdd_output,\n",
    "                        output_format=\"PARQUET\"\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    rdd_output =  ProcessingOutput(\n",
    "                    source=\"/opt/ml/processing/output/rdd\",\n",
    "                    output_name=\"dummy_output\",\n",
    "                    s3_upload_mode=\"EndOfJob\",\n",
    "                )\n",
    "\n",
    "    spark_processor = PySparkProcessor(\n",
    "        framework_version=\"3.1\",\n",
    "        image_uri = \"\",\n",
    "        role=_processing_job_role_arn,\n",
    "        instance_count=2,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        max_runtime_in_seconds=1200,\n",
    "\n",
    "    )\n",
    "\n",
    "    configuration = [\n",
    "        {\n",
    "                \"Classification\": \"spark-defaults\",\n",
    "                \"Properties\": {\n",
    "                    \"spark.driver.memory\": \"2g\",\n",
    "                    \"spark.executor.memory\": \"1g\",\n",
    "                    \"spark.executor.cores\":\"2\",\n",
    "                    \"spark.executor.instances\":\"10\"\n",
    "                },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    spark_processor.run(\n",
    "        submit_app=_submit_app,\n",
    "        submit_jars=[_jar_location],\n",
    "        job_name = _job_name,\n",
    "        arguments=[\n",
    "                \"--s3-input-path\", _s3_rdd_output,\n",
    "                \"--job-name\", _job_name,\n",
    "                \"--feature-group-arn\", _feature_group_arn\n",
    "            ],\n",
    "        inputs=[\n",
    "                rdd_input\n",
    "            ],\n",
    "        outputs=[\n",
    "                rdd_output\n",
    "            ],\n",
    "        configuration=configuration,\n",
    "        logs = False,\n",
    "        wait = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54afb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Source configuration \n",
    "redshift_secret = 'SecretForRedshiftSageMakerDemo2023'\n",
    "redshift_schema ='sagemakerdemo'\n",
    "\n",
    "# Target configuration - Feature store\n",
    "feature_group_prefix = 'redshift-sm-demo-4b-'\n",
    "\n",
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(SecretId=redshift_secret)\n",
    "_database_secrets = json.loads(response['SecretString'])\n",
    "_username = _database_secrets['username']\n",
    "_dbname = _database_secrets['dbname']\n",
    "_cluster_id = _database_secrets['dbClusterIdentifier']\n",
    "\n",
    "\n",
    "_redshift_role_arn = get_execution_role()\n",
    "_processing_job_role_arn = get_execution_role()\n",
    "\n",
    "_s3_bucket =sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335ca32-b5bc-43fc-8709-8fab55f28988",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can configure and launch Processing Job with Spark, please replace corresponding parameters with your owns.\n",
    "Here we configure Wait to True to check status of each job, OR you can make it False and check jobs on the SageMaker Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8d42d-9952-4107-95b5-8caad016bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "_job_name = \"redshift-dataset-to-feature-store-{}\".format(time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "_s3_rdd_output = \"s3://{}/processing-job/rdd/{}\".format(_s3_bucket,_job_name)\n",
    "\n",
    "_query_string = \"SELECT * FROM %s.dim_user\" % (redshift_schema)\n",
    "\n",
    "_feature_group_name = feature_group_prefix + 'users'\n",
    "_feature_group_arn = sagemaker.Session().describe_feature_group(_feature_group_name)['FeatureGroupArn']\n",
    "\n",
    "_submit_app = \"./code/processing_etl_user.py\"\n",
    "\n",
    "invoke_sagemaker_processing_job(_cluster_id,_dbname,_username,_query_string,_redshift_role_arn,_s3_rdd_output,_processing_job_role_arn,_submit_app,_jar_location,_job_name,_feature_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99952cb9-29d4-43c0-b653-49f52fa7635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_job_name = \"redshift-dataset-to-feature-store-{}\".format(time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "_s3_rdd_output = \"s3://{}/processing-job/rdd/{}\".format(_s3_bucket,_job_name)\n",
    "\n",
    "_query_string = \"SELECT * FROM %s.dim_place\" % (redshift_schema)\n",
    "\n",
    "_feature_group_name = feature_group_prefix + 'places'\n",
    "_feature_group_arn = sagemaker.Session().describe_feature_group(_feature_group_name)['FeatureGroupArn']\n",
    "\n",
    "_submit_app = \"./code/processing_etl_place.py\"\n",
    "\n",
    "invoke_sagemaker_processing_job(_cluster_id,_dbname,_username,_query_string,_redshift_role_arn,_s3_rdd_output,_processing_job_role_arn,_submit_app,_jar_location,_job_name,_feature_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6352d-9055-4456-bf12-9a946aa87258",
   "metadata": {},
   "outputs": [],
   "source": [
    "_job_name = \"redshift-dataset-to-feature-store-{}\".format(time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "_s3_rdd_output = \"s3://{}/processing-job/rdd/{}\".format(_s3_bucket,_job_name)\n",
    "\n",
    "_query_string = \"SELECT * FROM %s.fact_rating\" % (redshift_schema)\n",
    "\n",
    "_feature_group_name = feature_group_prefix + 'ratings'\n",
    "_feature_group_arn = sagemaker.Session().describe_feature_group(_feature_group_name)['FeatureGroupArn']\n",
    "\n",
    "_submit_app = \"./code/processing_etl_rating.py\"\n",
    "\n",
    "invoke_sagemaker_processing_job(_cluster_id,_dbname,_username,_query_string,_redshift_role_arn,_s3_rdd_output,_processing_job_role_arn,_submit_app,_jar_location,_job_name,_feature_group_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7318a-0a56-47ce-844d-e4d1fc0af35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
